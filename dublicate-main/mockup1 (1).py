# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypSFHTMvMRW_Fy6n4nQNi4EmfWEoCjcZ
"""

import io, uuid, os
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter, ImageChops
from google.colab import files
from IPython.display import display

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

import cv2

from transformers import BlipProcessor, BlipForConditionalGeneration

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using:", device)

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
).to(device)

print("Loading BLIP...")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base"
).to(device)
print("BLIP loaded successfully.")

def pil_from_bytes(b):
    return Image.open(io.BytesIO(b)).convert("RGB")

def save_image(img, prefix):
    name = f"{prefix}_{uuid.uuid4().hex[:8]}.png"
    path = os.path.join(OUT_DIR, name)
    img.save(path)
    return path

def generate_caption(img):
    inputs = processor(images=img, return_tensors="pt").to(device)
    out = blip_model.generate(**inputs, max_length=40)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

def apply_clahe_cv(img):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    l2 = clahe.apply(l)
    lab2 = cv2.merge((l2, a, b))
    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)

def auto_gamma_cv(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    mean = gray.mean() / 255.0
    gamma = 0.7 if mean < 0.4 else 0.9 if mean < 0.6 else 1.1 if mean > 0.8 else 1.0
    inv = 1 / gamma
    table = (np.array([(i/255)**inv * 255 for i in range(256)])).astype("uint8")
    return cv2.LUT(img, table)

def enhance_image(pil_img):
    img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    img_cv = apply_clahe_cv(img_cv)
    img_cv = auto_gamma_cv(img_cv)
    out = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    out = ImageEnhance.Sharpness(out).enhance(1.1)
    return out

def tile_texture(tex, size):
    w, h = size
    tw, th = tex.size
    canvas = Image.new("RGB", (w,h))
    for x in range(0, w, tw):
        for y in range(0, h, th):
            canvas.paste(tex, (x,y))
    return canvas

def apply_mockup(template, mask, fabric):
    W, H = template.size
    fabric_tile = tile_texture(fabric, (W,H))

    mask = mask.convert("L").resize((W,H))
    mask = mask.filter(ImageFilter.GaussianBlur(2))

    temp_rgba = template.convert("RGBA")
    fabric_rgba = fabric_tile.convert("RGBA")

    composite = Image.composite(fabric_rgba, temp_rgba, mask)

    # subtle shadow
    shadow = Image.new("RGBA", (W,H), (0,0,0,60))
    shadow_mask = mask.point(lambda p: int(p*0.2))
    shadow_layer = Image.new("RGBA",(W,H))
    shadow_layer.paste(shadow, (0,0), shadow_mask)

    final = Image.alpha_composite(composite, shadow_layer)
    return final.convert("RGB")

import io, uuid, os
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter, ImageChops
from google.colab import files
from IPython.display import display

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

import cv2
from transformers import BlipProcessor, BlipForConditionalGeneration

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

OUT_DIR = "fabric_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ================================
# MAIN BLOCK ‚Äî UPLOAD & PREDICT
# ================================
print("/content/fabric.png")
uploaded = files.upload()

fabric = None
for name, data in uploaded.items():
    fabric = pil_from_bytes(data)
    print("Loaded fabric:", name)

display(fabric)

print("\nüì§ Upload mockup template files:")
print("Upload: pillow.png, pillow_mask.png, tote.png, tote_mask.png, dress.png, dress_mask.png")

tpl_files = files.upload()

templates = {}
for name, data in tpl_files.items():
    templates[name] = pil_from_bytes(data)

print("Loaded templates:", list(templates.keys()))

# ------------------------------
# RUN COMPLETE PIPELINE
# ------------------------------
print("\nüìù Step 1 ‚Äî Description:")
caption = generate_caption(fabric)
print("‚Üí", caption)

print("\n‚ú® Step 2 ‚Äî Enhanced fabric:")
enhanced = enhance_image(fabric)
display(enhanced)
enhanced_path = save_image(enhanced, "enhanced")
print("Saved:", enhanced_path)

print("\nüé® Step 3 ‚Äî Generating Mockups:")
mockups = {}

pairs = [
    ("pillow.png", "pillow_mask.png", "Pillow"),
    ("tote.png", "tote_mask.png", "Tote Bag"),
    ("dress.png", "dress_mask.png", "Dress")
]

for tpl, mask, label in pairs:
    if tpl in templates and mask in templates:
        mock = apply_mockup(templates[tpl], templates[mask], enhanced)
        print(f"\nüßµ {label} Preview:")
        display(mock)
        out_path = save_image(mock, f"mockup_{label.lower().replace(' ','_')}")
        mockups[label] = out_path
        print("Saved:", out_path)
    else:
        print(f"‚ö†Ô∏è Missing {tpl} or {mask}")

print("\n‚úÖ ALL DONE!")
print("Outputs saved in folder:", OUT_DIR)

